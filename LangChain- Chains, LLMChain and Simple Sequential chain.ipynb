{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7614990c",
   "metadata": {},
   "source": [
    "# Chains in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bb3f1",
   "metadata": {
    "height": 30
   },
   "source": [
    "Sometimes when you are making complex applications it is beneficial to Chain Multiple LLMs or muliple Components together to make an application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad223012",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2eb1303",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#llm model\n",
    "llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e43ef3",
   "metadata": {},
   "source": [
    "We need a data that we will pass to model, so importing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b730ff5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcd77335",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n  Â I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1816528",
   "metadata": {
    "height": 30
   },
   "source": [
    "## LLM Chain:\n",
    "Simplest type of chain in which you Chain together Prompt and LLM model, you just combine them together to make a building block.\n",
    "After that just give input to this chain and get the output, in behind the scene input will be fit in prompt and prompt will be passed to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed3890d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8aa55158",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#for randomness in output setting temperature high\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411cb049",
   "metadata": {
    "height": 30
   },
   "source": [
    "The Prompt we will be Using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f72df33d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879b0c3",
   "metadata": {},
   "source": [
    "Making the Chain by specifying LLM and Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "763a5d0b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33899acb",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TechLap Inc.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Laptop\"\n",
    "#You can pass input will .run() and directly get output\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b11a21",
   "metadata": {},
   "source": [
    "## Sequential Chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523defe5",
   "metadata": {
    "height": 30
   },
   "source": [
    "In this chain we basically combine the multiple chain together to form a single chain.\n",
    "Output of one chain is input to second Chain.\n",
    "There are two types of Sequential Chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7363617",
   "metadata": {
    "height": 30
   },
   "source": [
    "#### 1- Simple Sequential chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeba8c5",
   "metadata": {
    "height": 30
   },
   "source": [
    "In this we have one input and one output for the each chain in this combination of the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c839c60a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a34cb1df",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09fcb4",
   "metadata": {
    "height": 30
   },
   "source": [
    "Chain 1: So our first Chanin will take the Product name and will suggest the Company name that makes that product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1999be29",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20067afa",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8310b4",
   "metadata": {
    "height": 30
   },
   "source": [
    "Chain 2: This chain will take output of Chain 1, means it will take company's name and will give discription for the Company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "573cc4f6",
   "metadata": {
    "height": 126
   },
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adf22d66",
   "metadata": {
    "height": 46
   },
   "outputs": [],
   "source": [
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afa26e",
   "metadata": {
    "height": 30
   },
   "source": [
    "Passing these LLM chains to SimpleSequentialChain() to make chain and Testing this Simple Sequential Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a13e0e07",
   "metadata": {
    "height": 77
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846475a6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mLaptopTech\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740825e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96568f1a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89614934",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f27557",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769a186",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea26da6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81e4f0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8f2bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cd9b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263c4b7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
