{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651a3cae",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cc7165",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#LLM we will be using \n",
    "llm_model= \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ba2ee",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now we will write a function that takes the input of the user and model, pass the input to model (which is our LLM) and return the response of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea0d117",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    #messagge---> User role means that this is the message of the user\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    #Passing message to model and getting response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        #no randomness in the output\n",
    "        temperature=0, \n",
    "    )\n",
    "    #returns the response\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f43978",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets test\n",
    "get_completion(\"What is 1200/60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bede7d1",
   "metadata": {
    "height": 30
   },
   "source": [
    "Lets say we have the email of the customer in someother language and we want to convert them to American English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06da80f6",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "#So customer email in Pirate English\n",
    "customer_email= \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf3439b",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "#we want it to be in American English so\n",
    "required_style= \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0924f39",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now we have to make a prompt by combining the Customer Email which is input and stle so that we can pass that prompt to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "135ffa06",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {required_style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b3c2230",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "#passing it to above function and getting response\n",
    "response= get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17a9730",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite upset that my blender lid came off and caused my smoothie to splatter all over my kitchen walls. Additionally, the warranty does not cover the cost of cleaning up the mess. Would you be able to assist me at this time, my friend?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51afc09",
   "metadata": {
    "height": 30
   },
   "source": [
    "# Langchain\n",
    "\n",
    "### What is it and Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5d19a",
   "metadata": {
    "height": 30
   },
   "source": [
    "LangChain is basically the Framework used for building applications with LLMs. \n",
    "Consider here in this example if you want now to respond user with American Pirate, for that you want to convert your written response which is written in American English to Pirate English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fc9d3",
   "metadata": {},
   "source": [
    "For that You will again write Prompt from very start to Convert this and then you will send. So here Langchain comes in the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26007a13",
   "metadata": {},
   "source": [
    "#### Prompts:\n",
    "With Langchain Prompts basically you can make Prompt Template that you can reuse again and again. It saves alot of time when you have application wher prompts are similar.\n",
    "\n",
    "So lets create Prompt Template with which we can convert these emails to the language and style we want with just giving the style and input email.\n",
    "\n",
    "We will have a genral prompt that we will be able to use for different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64dfb0fa",
   "metadata": {
    "height": 62
   },
   "outputs": [],
   "source": [
    "#importing ChatOpenAI with which we will make our LLM model first (by making its object)\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc047137",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0301', temperature=0.0, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fdd03c",
   "metadata": {
    "height": 30
   },
   "source": [
    "**Prompt Template for Translation of Emails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "906dfa78",
   "metadata": {
    "height": 130
   },
   "outputs": [],
   "source": [
    "#this is the template string that we will pass to form_template function of LangChain|\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c7b5548",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31612",
   "metadata": {},
   "source": [
    "Now using form_Template to make a Prompt Template from the string we have specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcac296b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24b78423",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e938b60",
   "metadata": {
    "height": 30
   },
   "source": [
    "As you can see from input_variables that it requires two variable style and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd097534",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d5b8f",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now lets specify Customer Conversion Style which will be American English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79256cf6",
   "metadata": {
    "height": 96
   },
   "outputs": [],
   "source": [
    "#as customers email will be in other language like English Pirate and we will convert it American English\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a8eb191",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "#now input text of email that the customer have sent\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e69d9",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now we will form the Prompt or the message by using the template of prompt we have made with form_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34460d5f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fef27500",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd27e4d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed5f95e5",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58f50c20",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a29a51",
   "metadata": {
    "height": 30
   },
   "source": [
    "#### Lets use same template to Convert Email from Servic Employee to Customer, with this you will get an Idea of what type of Abstraction LangChain Prompts Provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa3afdc1",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7521e4c",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5383",
   "metadata": {
    "height": 30
   },
   "source": [
    "See we are just changing the Input and Style rest, Prompt remains same which was prompt_template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f093acc6",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ab1d789",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, me hearty customer! I be sorry to inform ye that the warranty be not coverin' the expenses o' cleaning yer galley, as it be yer own fault fer misusin' yer blender by forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds be in yer favor!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c085807",
   "metadata": {
    "height": 30
   },
   "source": [
    "It is benficial when in Complex Application where you have Long Prompts similar to each other so you have to write again and again. \n",
    "\n",
    "So with this you can just reuse them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c741a0",
   "metadata": {
    "height": 30
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
